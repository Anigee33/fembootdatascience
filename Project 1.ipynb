{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "a=np.random.randint(0,2,20)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ z=a^3 +sin(a) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.84147098, 1.84147098, 0.        , 1.84147098, 0.        ,\n",
       "        1.84147098, 1.84147098, 0.        , 1.84147098, 1.84147098,\n",
       "        0.        , 1.84147098, 1.84147098, 0.        , 0.        ,\n",
       "        1.84147098, 0.        , 0.        , 0.        , 0.        ])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=[]\n",
    "def f(n):\n",
    "    z.append(n**3+np.sin(n))\n",
    "    return z\n",
    "f(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.84, 1.84, 0.  , 1.84, 0.  , 1.84, 1.84, 0.  , 1.84, 1.84, 0.  ,\n",
       "       1.84, 1.84, 0.  , 0.  , 1.84, 0.  , 0.  , 0.  , 0.  ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rounds to 2 dp and converts list to a 1 dimensional array\n",
    "b=np.round(z,2)\n",
    "b=b.reshape(20,)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(a,b)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>xVel1</th>\n",
       "      <th>yVel1</th>\n",
       "      <th>xA1</th>\n",
       "      <th>yA1</th>\n",
       "      <th>xS1</th>\n",
       "      <th>yS1</th>\n",
       "      <th>xC1</th>\n",
       "      <th>yC1</th>\n",
       "      <th>...</th>\n",
       "      <th>yVel200</th>\n",
       "      <th>xA200</th>\n",
       "      <th>yA200</th>\n",
       "      <th>xS200</th>\n",
       "      <th>yS200</th>\n",
       "      <th>xC200</th>\n",
       "      <th>yC200</th>\n",
       "      <th>nAC200</th>\n",
       "      <th>nS200</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-489.28</td>\n",
       "      <td>-658.11</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.93</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>112</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-540.61</td>\n",
       "      <td>-670.93</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>-4.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>112</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>379.83</td>\n",
       "      <td>-521.18</td>\n",
       "      <td>1.86</td>\n",
       "      <td>7.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>888.21</td>\n",
       "      <td>-146.53</td>\n",
       "      <td>-2.67</td>\n",
       "      <td>-18.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-452.43</td>\n",
       "      <td>-632.15</td>\n",
       "      <td>2.66</td>\n",
       "      <td>-2.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18008</th>\n",
       "      <td>-574.72</td>\n",
       "      <td>-176.30</td>\n",
       "      <td>-5.15</td>\n",
       "      <td>-5.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18009</th>\n",
       "      <td>-192.91</td>\n",
       "      <td>-997.61</td>\n",
       "      <td>5.70</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.06</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18010</th>\n",
       "      <td>273.54</td>\n",
       "      <td>841.54</td>\n",
       "      <td>-4.74</td>\n",
       "      <td>-5.54</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>...</td>\n",
       "      <td>5.16</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>0.51</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18011</th>\n",
       "      <td>1096.38</td>\n",
       "      <td>-270.36</td>\n",
       "      <td>-6.87</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.03</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18012</th>\n",
       "      <td>237.53</td>\n",
       "      <td>-712.97</td>\n",
       "      <td>1.98</td>\n",
       "      <td>7.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>5.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18013 rows × 2401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x1      y1  xVel1  yVel1   xA1   yA1   xS1   yS1   xC1   yC1  ...  \\\n",
       "0      -489.28 -658.11   2.51   3.28  0.00  0.00 -0.26  0.35  0.93 -0.37  ...   \n",
       "1      -540.61 -670.93  -1.02  -4.76  0.00  0.00 -0.06  0.44  0.24 -0.97  ...   \n",
       "2       379.83 -521.18   1.86   7.31  0.00  0.00  0.00  0.00  0.00  0.00  ...   \n",
       "3       888.21 -146.53  -2.67 -18.10  0.00  0.00  0.00  0.00  0.00  0.00  ...   \n",
       "4      -452.43 -632.15   2.66  -2.63  0.00  0.00 -0.12  0.42  0.01 -1.00  ...   \n",
       "...        ...     ...    ...    ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "18008  -574.72 -176.30  -5.15  -5.88  0.00  0.00  0.00  0.00  0.00  0.00  ...   \n",
       "18009  -192.91 -997.61   5.70  -0.92 -0.14 -0.29  0.00  0.00  0.19  0.25  ...   \n",
       "18010   273.54  841.54  -4.74  -5.54 -0.73 -0.69  0.00  0.00  0.51 -0.86  ...   \n",
       "18011  1096.38 -270.36  -6.87  -0.65 -0.56  0.03  0.00  0.00 -0.32  0.05  ...   \n",
       "18012   237.53 -712.97   1.98   7.16  0.00  1.00  0.00  0.00  0.00  0.00  ...   \n",
       "\n",
       "       yVel200  xA200  yA200  xS200  yS200  xC200  yC200  nAC200  nS200  \\\n",
       "0        -4.17   0.00   0.00   0.33   0.29  -0.87  -0.50     112     40   \n",
       "1        -4.43   0.00   0.00  -0.15   0.41  -0.46  -0.89     112      9   \n",
       "2         1.33   0.00   0.00   0.00   0.00   0.00   0.00       4      0   \n",
       "3        -2.19   0.00   0.00   0.00   0.00   0.00   0.00       3      1   \n",
       "4         2.54   0.00   0.00   0.00   0.00  -1.00  -0.10     112      0   \n",
       "...        ...    ...    ...    ...    ...    ...    ...     ...    ...   \n",
       "18008    -8.51   0.00   0.00   0.00   0.00   0.00   0.00       0      0   \n",
       "18009    -2.06  -0.18  -0.27   0.00   0.00  -0.10   0.04       0      0   \n",
       "18010     5.16  -0.95   0.32   0.00   0.00  -0.86   0.51       6      1   \n",
       "18011    -0.18  -0.56  -0.04   0.00   0.00  -0.32   0.03      71      0   \n",
       "18012     5.59   0.00   1.00   0.00   0.00   0.00   0.00      77      0   \n",
       "\n",
       "       Class   \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           0  \n",
       "4           1  \n",
       "...       ...  \n",
       "18008       0  \n",
       "18009       0  \n",
       "18010       1  \n",
       "18011       1  \n",
       "18012       0  \n",
       "\n",
       "[18013 rows x 2401 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "model1=LogisticRegression()\n",
    "df1=pd.read_csv(\"swarm_train_data.csv\")\n",
    "df2=pd.read_csv(\"swarm_test_data.csv\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>xVel1</th>\n",
       "      <th>yVel1</th>\n",
       "      <th>xA1</th>\n",
       "      <th>yA1</th>\n",
       "      <th>xS1</th>\n",
       "      <th>yS1</th>\n",
       "      <th>xC1</th>\n",
       "      <th>yC1</th>\n",
       "      <th>...</th>\n",
       "      <th>xVel200</th>\n",
       "      <th>yVel200</th>\n",
       "      <th>xA200</th>\n",
       "      <th>yA200</th>\n",
       "      <th>xS200</th>\n",
       "      <th>yS200</th>\n",
       "      <th>xC200</th>\n",
       "      <th>yC200</th>\n",
       "      <th>nAC200</th>\n",
       "      <th>nS200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-310.36</td>\n",
       "      <td>94.44</td>\n",
       "      <td>-3.49</td>\n",
       "      <td>-3.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.20</td>\n",
       "      <td>-5.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>875.60</td>\n",
       "      <td>645.34</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>8.96</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.15</td>\n",
       "      <td>9.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>257.97</td>\n",
       "      <td>650.78</td>\n",
       "      <td>6.74</td>\n",
       "      <td>6.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>14.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>995.13</td>\n",
       "      <td>236.26</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>-4.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.37</td>\n",
       "      <td>-2.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-782.80</td>\n",
       "      <td>-163.54</td>\n",
       "      <td>12.69</td>\n",
       "      <td>13.12</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-2.67</td>\n",
       "      <td>...</td>\n",
       "      <td>9.68</td>\n",
       "      <td>-1.86</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.04</td>\n",
       "      <td>-1.74</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>-1400.85</td>\n",
       "      <td>-198.91</td>\n",
       "      <td>-7.46</td>\n",
       "      <td>-2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.58</td>\n",
       "      <td>-7.05</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>-535.60</td>\n",
       "      <td>-583.25</td>\n",
       "      <td>-11.99</td>\n",
       "      <td>2.31</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.25</td>\n",
       "      <td>-2.37</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.36</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-3.36</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2.58</td>\n",
       "      <td>0.71</td>\n",
       "      <td>112</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>973.72</td>\n",
       "      <td>110.21</td>\n",
       "      <td>-1.51</td>\n",
       "      <td>-8.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.58</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6001</th>\n",
       "      <td>1073.24</td>\n",
       "      <td>-844.54</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>8.48</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>9.33</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6002</th>\n",
       "      <td>494.28</td>\n",
       "      <td>-521.74</td>\n",
       "      <td>4.36</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.28</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6003 rows × 2400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1      y1  xVel1  yVel1   xA1   yA1   xS1   yS1   xC1   yC1  ...  \\\n",
       "0     -310.36   94.44  -3.49  -3.04  0.00  0.00  0.00  0.00  0.00  0.00  ...   \n",
       "1      875.60  645.34  -0.32   8.96 -0.04  1.00  0.00  0.00 -0.04  1.00  ...   \n",
       "2      257.97  650.78   6.74   6.57  0.00  0.00  0.00  0.00  0.00  0.00  ...   \n",
       "3      995.13  236.26  -0.76  -4.47  0.00  0.00  0.00  0.00  0.00  0.00  ...   \n",
       "4     -782.80 -163.54  12.69  13.12 -0.19  0.98  0.00  0.00  0.17 -2.67  ...   \n",
       "...       ...     ...    ...    ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "5998 -1400.85 -198.91  -7.46  -2.34  0.00  0.00  0.00  0.00  0.00  0.00  ...   \n",
       "5999  -535.60 -583.25 -11.99   2.31 -0.88  0.48  1.02  1.53  1.25 -2.37  ...   \n",
       "6000   973.72  110.21  -1.51  -8.88  0.00  0.00  0.00  0.00  0.00  0.00  ...   \n",
       "6001  1073.24 -844.54  -0.31   8.48 -0.04  1.00  0.00  0.00 -0.04  1.00  ...   \n",
       "6002   494.28 -521.74   4.36   0.43  0.09  0.04  0.00  0.00  0.00  0.00  ...   \n",
       "\n",
       "      xVel200  yVel200  xA200  yA200  xS200  yS200  xC200  yC200  nAC200  \\\n",
       "0       -3.20    -5.30   0.00   0.00   0.00   0.00   0.00   0.00       0   \n",
       "1        0.15     9.21   0.02   1.00   0.00   0.00   0.02   1.00      15   \n",
       "2       -2.17    14.59   0.00   1.00   0.00   0.00   0.00   0.00       3   \n",
       "3       -3.37    -2.95   0.00   0.00   0.00   0.00   0.00   0.00       0   \n",
       "4        9.68    -1.86   0.69   0.73   0.00   0.00   2.04  -1.74      59   \n",
       "...       ...      ...    ...    ...    ...    ...    ...    ...     ...   \n",
       "5998     0.58    -7.05  -0.33  -0.94   0.00   0.00  -0.10  -0.99       3   \n",
       "5999   -12.36     8.69   0.84   0.55  -3.36   0.43   2.58   0.71     112   \n",
       "6000    -4.58    -4.00   0.00   0.00   0.00   0.00   0.00   0.00       2   \n",
       "6001    -0.19     9.33  -0.02   1.00   0.00   0.00  -0.02   1.00      11   \n",
       "6002    -3.28    -0.78  -0.15  -0.28   0.00   0.00  -0.01   0.00       2   \n",
       "\n",
       "      nS200  \n",
       "0         0  \n",
       "1         5  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "5998      1  \n",
       "5999     15  \n",
       "6000      0  \n",
       "6001      2  \n",
       "6002      0  \n",
       "\n",
       "[6003 rows x 2400 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6048409482040749"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First model\n",
    "\n",
    "X=df1['xVel1']\n",
    "X=X.values\n",
    "X=X.reshape(-1,1)\n",
    "y=df1.iloc[:,-1]\n",
    "model1.fit(X,y)\n",
    "model1.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7102092932881807"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model1 with more data\n",
    "\n",
    "X1=df1.loc[:,['x2','xVel2','nAC2','nS2' ]]\n",
    "X1=X1.values\n",
    "y=df1.iloc[:,-1]\n",
    "model1.fit(X1,y)\n",
    "model1.score(X1,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6048409482040749"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#second model (first hyperparameterized model)\n",
    "\n",
    "model2=LogisticRegression(random_state=0,solver ='liblinear')\n",
    "X=df1['xVel1']\n",
    "X=X.values\n",
    "X=X.reshape(-1,1)\n",
    "y=df1.iloc[:,-1]\n",
    "model2.fit(X,y)\n",
    "model2.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'C': 0.001, 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\ANIEMA E\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\",'elasticnet','none']}\n",
    "\n",
    "logreg_cv=GridSearchCV(model1,grid,cv=3)\n",
    "logreg_cv.fit(X,y)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6050074945872426"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#third model using best parameters from GridSearchCV\n",
    "\n",
    "model3=LogisticRegression(C= 0.001,penalty='l2')\n",
    "X=df1['xVel1']\n",
    "X=X.values\n",
    "X=X.reshape(-1,1)\n",
    "y=df1.iloc[:,-1]\n",
    "model3.fit(X,y)\n",
    "model3.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict class label of test data set using model1\n",
    "\n",
    "X2=df2.iloc[:,[24,26,34,35]]\n",
    "X2=X2.values\n",
    "y_pred=model1.predict(X2)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict class label of test data set using model3\n",
    "\n",
    "X2=df2.iloc[:,24]\n",
    "X2=X2.values\n",
    "X2=X2.reshape(-1,1)\n",
    "y_pred2=model3.predict(X2)\n",
    "y2=y_pred2.reshape(-1,1)\n",
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.604 (0.013)\n"
     ]
    }
   ],
   "source": [
    "#Performance (Accuracy) of model1: my best model\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "scores = cross_val_score(model1, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-639aace94072>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the confusion matrix, I need the class label of the test dataset which will act as my y_test. \n",
    "Since this is absent in test data set provided, the arguments are not complete to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
